\documentclass[10pt]{article}

\usepackage{amsmath}
\usepackage{textcomp}
\usepackage[top=0.8in, bottom=0.8in, left=0.8in, right=0.8in]{geometry}
% add other packages here
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[font=scriptsize]{subcaption}
\usepackage[font=scriptsize]{caption}

% put your group number and names in the author field
\title{\bf Exercise 2: A Reactive Agent for the Pickup and Delivery Problem}
\author{Group \textnumero 23: Jean-Thomas FURRER, Emily HENTGEN}

% the report should not be longer than 3 pages

\begin{document}
\maketitle

\section{Problem Representation}

\subsection{Representation Description}
% describe how you design the state representation, the possible actions, the reward table and the probability transition table

In our implementation, a state is described by two different attributes:
the current city of the agent and the destination city of the task. Implicitly, whether there is an available task in this city is indicated by the destination city, and a \textit{null} value means there is no available task.

An action is described by three different attributes: whether it consists of a pick up or just a move between two cities, the departure city and the destination city.

The reward table consists of a table mapping a \textit{State} to a numerical reward. Similarly, the probability transition table maps a \textit{State} to the probability of arriving at this \textit{State}.
For a \textit{State} with an available task, this is simply the probability of having an available task in the current city to the destination city, given by the task distribution.
For a \textit{State} with no available task, that is, a state with a \textit{null} destination city, this is the probability of having no task in this city, also given by the task distribution.

\subsection{Implementation Details}
% describe the implementation details of the representations above and the implementation details of the reinforcement learning algorithm you implemented

The agent behaviour is implemented in the \textit{ReactiveTemplate} class. For comparison purposes, there are two additional dummy agents: a random agent who randomly travels between cities, and randomly picks the available task or not (implemented in the \textit{RandomTemplate} class) and a routine agent, who also randomly picks up tasks, and otherwise follows a randomly predetermined itinerary (implemented in the \textit{DummyTemplate} class).
The two classes \textit{State} and \textit{Action} (\textit{template.Action}) implement a state respectively an action representation, both defined by their respective above-mentioned attributes.

The main data structures are implemented as \textit{HashMap}s, so that accessing elements is fast. The data structure representing the set of all states is a \textit{List}. The additional data structure \textit{statesForCity} is not strictly necessary, but speeds up the retrieval of all states associated to a given city (instead of looping through the entire \textit{allStates} list).
\begin{itemize}
\itemsep 1mm 
\item[]\textbf{rewards}  \textit{HashMap\textless template.Action, Double\textgreater}
\item[]\textbf{probabilities} \textit{HashMap\textless State, Double\textgreater}
\item[]\textbf{bestActions} \textit{HashMap\textless State, Action\textgreater}
\item[]\textbf{bestValues} \textit{HashMap\textless State, Double\textgreater}
\item[]\textbf{allStates} \textit{List\textless State\textgreater}
\item[]\textbf{statesForCity} \textit{HashMap\textless City, List\textless State\textgreater \textgreater}
\end{itemize}

\noindent
The redundant attributes between a \textit{State} and an \textit{Action} (in particular the current city/departure city) make the use of the HashMaps more practical: the keys, which conceptually consists of a \textit{(State, Action)} pair are here either a \textit{State} or an \textit{Action}, which simplifies the overall implementation.

\begin{algorithm}[t]
\caption{Learning Strategy (Value iteration)}
\begin{algorithmic}[1]
\Statex \textsc{Input}
\Statex \hspace{\algorithmicindent} \textit{rewards} \Comment{the table mapping an action to its expected reward}
\Statex \hspace{\algorithmicindent} \textit{probabilities} \Comment{the table mapping a state to the probability of being in this state}
\Statex \hspace{\algorithmicindent} \textit{allStates} \Comment{the set of all possible states}
\Statex \hspace{\algorithmicindent} \textit{statesForCity} \Comment{the table mapping a city to its set of possible states}
\Statex \hspace{\algorithmicindent} \textit{discountFactor} \Comment{the probability an agent picks up a task}
\Statex \textsc{Output}
\Statex \hspace{\algorithmicindent} \textit{bestMoves} \Comment{the table mapping a state to the next best action and the associated value}
\Statex
\State $hasConverged \gets false$
\State initialize $bestMoves$ to an empty table
\Statex
\While{not $hasConverged$}
\For{$state$ in $allStates$}
	\State $maxQ \gets -\infty$
	\For{$action$ in $state.actions$}
		\State $acc \gets 0$
		\For{$nextState$ in $statesForCity[action.cityTo]$}
			\State $acc = acc + probabilities[nextState] * bestMoves[nextState].value$
		\EndFor
		\State $Q = rewards[action] + discountFactor * acc$
		\If{$Q > maxQ$} 
			\State $maxQ \gets Q$
			\State $bestMoves[state] = (action, Q)$
			\State $hasConverged \gets false$
		\EndIf
	\EndFor
\EndFor
\EndWhile
\State \Return bestMoves
\end{algorithmic}
\end{algorithm}





\section{Results}
% in this section, you describe several results from the experiments with your reactive agent

\subsection{Experiment 1: Discount factor}
% the purpose of this experiment is to understand how the discount factor influences the result

\subsubsection{Setting}
% you describe how you perform the experiment (you also need to specify the configuration used for the experiment)
In this setting, three different reactive agents with a discount factor of 0.99, 0.5 and 0.0 respectively are tested against each other.

\subsubsection{Observations}
% you describe the experimental results and the conclusions you inferred from these results

\begin{figure}[h!]
\centering
\begin{subfigure}[t]{0.47\textwidth}
\captionsetup{width=1.0\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, scale=0.2]{images/"reactive_discount_factor_099_050_000".png}
\caption{Reactive agent 1 with discount factor = 0.99 (blue) -- reactive agent 2 with discount factor = 0.50 (red) -- reactive agent 3 with discount factor = 0.0}
\end{subfigure}
\caption{Discount factor influence on the reward per km}
\label{discount factor}
\end{figure}

On average, the higher the discount factor, the higher the performance of the reactive agent: the reactive agent with a discount factor of 0.99 has in the long run a reward per km greater than all the other agents.
The agent which displays the lowest performance is the one with discount factor 0.0, that is, the one which optimizes its behaviour for an immediate reward.
These results however can often be observed only after a significant number of steps, due to important fluctuations between the average reward per km of the reactive agents.
At some points during the simulation for instance, the reactive agent with discount factor 0.5 may outperform the reactive agent with discount factor 0.99, before the tendency eventually stabilizes (Figure \ref{discount factor}).

\subsection{Experiment 2: Comparisons with dummy agents}
% you compare the results of your agent with two dummy agents: the random agent that was already given in the starter files and another dummy agent that you define and create. You should report the results from the simulations using the topologies given in the starter files and optionally, additional topologies that you create.

\subsubsection{Setting}
% you describe how you perform the experiment and you describe the dummy agent you created (you also need to specify the configuration used for the experiment)
Here, a reactive agent with discount factor of 0.85 is first tested against a random agent and a routine agent who both accept tasks with probability 0.85.
Two reactive agents with discount factors 0.99 and 0.5 are then tested against two random agents and two routine agents accepting tasks with probability 0.99 or 0.5.

\subsubsection{Observations}
% elaborate on the observed results

\begin{figure}[h!]
\centering
\begin{subfigure}[t]{0.47\textwidth}
\captionsetup{width=1.0\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, scale=0.2]{images/random_085_routine_085_reactive_085".png}
\caption{Reactive agent with discount factor = 0.85 (green) against random dummy agent with discount factor = 0.85 (blue) and routine dummy agent with discount factor = 0.85 (red)}
\label{reactive vs dummies a}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.47\textwidth}
\captionsetup{width=1.0\textwidth}
\includegraphics[trim={0cm 0cm 0cm 0cm},clip, scale=0.2]{images/"random_100_routine_100_reactive_099_reactive_050_routine_050_random_050".png}
\caption{Reactive agent with discount factor = 0.99 (green) against another reactive agent with discount factor = 0.5 (yellow), 
a random agent with discount factor = 1.0 (blue),
a dummy agent with discount factor = 1.0 (red),
a dummy agent with discount factor = 0.5 (magenta) and 
a random agent with discount factor = 0.5 (cyan)}
\label{reactive vs dummies b}
\end{subfigure}
\caption{Reactive agents performance compared to dummy agents performance}
\label{reactive vs dummies}
\end{figure}

\noindent
The reactive agent displays a higher performance than both the routine dummy agent and the random dummy agent when the discount factor respectively the probability they accept an available task is 0.85 for all three agents.
Although the reward per km of both dummy agents fluctuates over time, their performance is rather similar in this case (Figure \ref{reactive vs dummies a}).

When both the random dummy agent and the routine dummy agent systematically accept an available task, the reactive agent with discount factor 0.99 as well as the one with discount factor 0.5 still perform better.
It is the random agent which accepts tasks with probability 0.5 which displays the lowest performance, but the random agent which systematically accepts task turns out to have a reward per km rather close to the one of the reactive agent with discount factor 0.5 (Figure \ref{reactive vs dummies b}).\\

\noindent
In the long run, a high discount factor tends to increase the reward per km of a reactive agent, since this agent seeks an optimal strategy that gives an important weight to the future.
On the other hand, the performance of a reactive agent with a low discount factor, which optimizes for immediate rewards, comes close to the one of a random agent which systematically accepts tasks. The learning performed beforehand by a reactive agent still gives him a slight advantage over a random agent though.

\end{document}